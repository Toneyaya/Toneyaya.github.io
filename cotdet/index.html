<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="">
    <meta name="author" content="Jiajin Tang*,
                                Ge Zheng*,
                                Jingyi Yu,
                                Sibei Yang">

    <title>CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">CoTDet</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Affordance Knowledge Prompting for Task Driven Object Detection</h1>
<!--    <h2>TensoRF: Tensorial Radiance Fields</h2>-->
        <h3 class="nerf_subheader_v2">Accepted by ICCV 2023</h3>
<!--            <p class="abstract">A compact and efficent scene representation</p>-->
    <hr>
    <p class="authors">
        <a href="https://Toneyaya.github.io/"> Jiajin Tang*</a>,
        <a> Ge Zheng*</a>,
        <a href="https://sist.shanghaitech.edu.cn/2020/0707/c7499a53862/page.htm"> Jingyi Yu</a>,
        <a href="https://faculty.sist.shanghaitech.edu.cn/"> Sibei Yang✉</a>

    </p>

    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>denotes equal contribution.</div>

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/">Paper</a>
        <a class="btn btn-primary" href="https://github.com/Toneyaya/CoTDet">Code (Coming Soon)</a>
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Our insights</h2>
        </br></br>
        <div class="vcontainer" style="text-align: center;">
            <img width=80% height=80% center src="./img/intro.png" alt="Dask Driven Object Detection with visual affordance knowledge">
        </div>
    </div>
    </br>
<!--<hr>-->
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Task driven object detection aims to detect object instances suitable for affording a task in an image. Its challenge
                lies in object categories available for the task being too diverse to be limited to a closed set of object vocabulary
                for traditional object detection.
                Simply mapping categories and visual features of common objects to the task cannot address the challenge.
                In this paper, we propose to explore fundamental affordances rather than object categories, i.e., common attributes that
                enable different objects to accomplish the same task. Moreover, we propose a novel multi-level chain-of-thought
                prompting (MLCoT) to extract the affordance knowledge from large language models, which contains multi-level reasoning
                steps from task to object examples to essential visual attributes with rationales.
                Furthermore, to fully exploit knowledge to benefit object recognition and localization, we propose a
                knowledge-conditional detection framework, namely CoTDet. It conditions the detector from the knowledge to generate
                object queries and regress boxes.
                Experimental results demonstrate that our CoTDet outperforms state-of-the-art methods consistently and significantly
                (+15.6 box AP and +14.8 mask AP) and can generate rationales for why objects are detected to afford the task.
            </p>
 
            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Given a task (e.g., “open parcel”) and an image, task driven object detection requires detecting a set of objects most preferred to afford the
                task.
                Note that the target objects indicated by the task are not fixed in quantity and category, which may vary with changes
                in the image scene. In contrast, traditional object detection detects objects of fixed categories while referring image
                grounding localizes unambiguous objects.
            </p>


            <div class="columns-5 w-row">
                <img src="img/cotdet.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text">
                The framework of our proposed CoTDet is shown in Figure. First, we introduce the problem definition
                and image and text encoders. Second, we acquire visual affordance knowledge from large language
                models (LLMs) by leveraging the multi-level chain-of-thought prompting and aggregation. Next,
                we present the knowledge-conditional decoder that conditions acquired knowledge to detect and segment suitable objects. 
                Finally, we introduce the loss functions.
            </p>
        </div>
    </div>



     </br></br>
        <div class="section">
            <s2>Performance</s2>

            <p class="paragraph-3 nerf_text">
                As shown in the tables below, we compare TempCD with state-of-the-art methods on four benchmarks. CoTDet
                consistently outperforms state-of-the-art methods on all tasks.
            </p>

            <h2 class="grey-heading_nerf">
                Main Results
            </h2>
            <p class="paragraph-3 nerf_text">
                Compared to TOIST, our CoTDet achieves significant performance improvement (15.6% mAPbox and 14.8% mAPmask), which
                demonstrates the effectiveness of our task-relevant knowledge acquisition and utilization. Compared to the two-stage
                method GGNN, we achieve 24.3% mAPbox and 21.2% mAPmask performance gain, which demonstrates the importance of leveraging
                the visual affordance knowledge rather than purely visual context information.
            </p>
            <div class="columns-5 w-row">
                <img src="img/e1.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>
            <p class="paragraph-3 nerf_text">
            Our CoTDet significantly improves the detection and segmentation performance on task4 (get potatoes out of fire), task6
            (get lemon out of tea) and task7 (dig hole), achieving approximately 20% mAP improvement on both benchmarks. These tasks
            face the common challenge of the wide variety of targets’ categories and visual appearances, which is hardly dealt with
            by methods like [19, 41] that merely learn the mapping between tasks and objects’ categories and visual features. In
            contrast, our method explicitly acquires the visual affordance knowledge of tasks to detect rare objects and avoid
            overfitting to common objects, outperforming significantly in these tasks. In addition, for those less challenging tasks
            with a few ground-truth object categories, we still achieve approximately 8% mAP improvement, demonstrating the
            effectiveness of conditioning on visual affordances to object localization.
            </p>
            <div class="columns-5 w-row">
                <img src="img/e2.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <h2 class="grey-heading_nerf">
                Visualization
            </h2>
            <p class="paragraph-3 nerf_text">
                Here we visualize several qualitative results. For (a), no objects in the image should
                be selected to “get lemon out of tea”. Our model can successfully return the empty set, while TOIST detects the french
                fry that is one of the salient objects in the image as the tool. Similarly, as knives are uncommon for “opening bottle
                of beer”, the knife in (b) is challenging for TOIST to identify and locate. Guided by the visual affordance of “sharp
                blade with a pointed end”, our model correctly localizes and selects the sharp knife.
            </p>
            <div class="columns-5 w-row">
                <img src="img/vz.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text nerf_results_text">
                The (c) and (d) demonstrate effectiveness without MLCOT or knowledge-conditional denoising training (KDN). With visual
                affordance knowledge obtained by directly asking LLMs, our model relies solely on matching with the single knowledge
                unit, which incorrectly detects the trunk in (c) and misses the knife in (d). The former trunk is easily confused with
                objects that are “flat, broad with a handle”, while the latter knife is ignored because its visual attributes of
                straight mismatch the single knowledge unit that includes “curved or angled”. Furthermore, without KDN, our detector
                lacks explicit guidance, leading to inaccurate detection in challenging scenes. Specifically, the glove in (c) and the
                knife in (d) are not detected successfully, and the packing line in (d) is mistakenly detected.
            </p>


<!-- 
            </br></br></br>
            <s2> Acknowledgements </s2>
            <hr>
            <p class="paragraph-3 nerf_text">
                We would like to thank .
            </p> -->

        </br></br>
<!--    <div class="section">-->
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @INPROCEEDINGS{TangICCV2023,
              author = {Jiajin Tang and Ge Zheng and Sibei Yang},
              title = {Temporal Collection and Distribution for Referring Video Object Segmentation},
              booktitle = {ICCV},
              month = {October},
              year = {2023}
            }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from NeRF.
        </p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
