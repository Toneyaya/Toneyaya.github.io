<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="">
    <meta name="author" content="Jiajin Tang*,
                                Ge Zheng*,
                                Jingyi Yu,
                                Sibei Yang">

    <title>CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">CoTDet</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Affordance Knowledge Prompting for Task Driven Object Detection</h1>
<!--    <h2>TensoRF: Tensorial Radiance Fields</h2>-->
        <h3 class="nerf_subheader_v2">Accepted by ICCV 2023</h3>
<!--            <p class="abstract">A compact and efficent scene representation</p>-->
    <hr>
    <p class="authors">
        <a href="https://Toneyaya.github.io/"> Jiajin Tang*</a>,
        <a> Ge Zheng*</a>,
        <a href="https://sist.shanghaitech.edu.cn/2020/0707/c7499a53862/page.htm"> Jingyi Yu</a>,
        <a href="https://faculty.sist.shanghaitech.edu.cn/"> Sibei Yang✉</a>

    </p>

    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Joint first authors.</div>

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/">Paper</a>
        <a class="btn btn-primary" href="https://github.com/Toneyaya/CoTDet">Code (Coming Soon)</a>
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Our insights</h2>
        </br></br>
        <div class="vcontainer" style="text-align: center;">
            <img width=80% height=80% center src="./img/intro.png" alt="Dask Driven Object Detection with visual affordance knowledge">
        </div>
    </div>
    </br>
<!--<hr>-->
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Task driven object detection aims to detect object instances suitable for affording a task in an image. Its challenge
                lies in object categories available for the task being too diverse to be limited to a closed set of object vocabulary
                for traditional object detection.
                Simply mapping categories and visual features of common objects to the task cannot address the challenge.
                In this paper, we propose to explore fundamental affordances rather than object categories, i.e., common attributes that
                enable different objects to accomplish the same task. Moreover, we propose a novel multi-level chain-of-thought
                prompting (MLCoT) to extract the affordance knowledge from large language models, which contains multi-level reasoning
                steps from task to object examples to essential visual attributes with rationales.
                Furthermore, to fully exploit knowledge to benefit object recognition and localization, we propose a
                knowledge-conditional detection framework, namely CoTDet. It conditions the detector from the knowledge to generate
                object queries and regress boxes.
                Experimental results demonstrate that our CoTDet outperforms state-of-the-art methods consistently and significantly
                (+15.6 box AP and +14.8 mask AP) and can generate rationales for why objects are detected to afford the task.
            </p>
 
            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Given a task (e.g., “open parcel”) and an image, task driven object detection requires detecting a set of objects most preferred to afford the
                task.
                Note that the target objects indicated by the task are not fixed in quantity and category, which may vary with changes
                in the image scene. In contrast, traditional object detection detects objects of fixed categories while referring image
                grounding localizes unambiguous objects.
            </p>


            <div class="columns-5 w-row">
                <img src="img/cotdet.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text">
                The framework of our proposed CoTDet is shown in Figure. First, we introduce the problem definition
                and image and text encoders. Second, we acquire visual affordance knowledge from large language
                models (LLMs) by leveraging the multi-level chain-of-thought prompting and aggregation. Next,
                we present the knowledge-conditional decoder that conditions acquired knowledge to detect and segment suitable objects. 
                Finally, we introduce the loss functions.
            </p>
        </div>
    </div>



    </br></br>
    <div class="section">
        <s2>Performance</s2>
      
        <p class="paragraph-3 nerf_text">
            Note that, unlike concurrent works <a href="https://alexyu.net/plenoxels/"> Plenoxels</a>
            and  <a href="https://nvlabs.github.io/instant-ngp/"> Instant-ngp</a>
            that require customized CUDA kernels, our model’s
            efficiency gains are obtained using a standard PyTorch implementation.
        </p>

        <h2 class="grey-heading_nerf">
            Main Results
        </h2>
        <p class="paragraph-3 nerf_text">
            
        </p>

        <div class="columns-5 w-row">
            <img src="img/performance.png" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>


        <h2 class="grey-heading_nerf">
            Visualization
        </h2>
        <p class="paragraph-3 nerf_text">
            Here we visualize 
        </p>
        <div class="columns-5 w-row">
            <img src="img/vz1.jpg" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>

        <p class="paragraph-3 nerf_text nerf_results_text">
            We can also 
        </p>
        <div class="columns-5 w-row">
            <img src="img/vz2.jpg" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>


        </br></br></br>
        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            We would like to thank .
        </p>

        </br></br>
<!--    <div class="section">-->
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @INPROCEEDINGS{TangICCV2023,
              author = {Jiajin Tang and Ge Zheng and Jingyi Yu and Sibei Yang},
              title = {CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection},
              booktitle = {ICCV},
              year = {2023}
            }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from NeRF.
        </p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
