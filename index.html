<!DOCTYPE html>
<html>
<head>
    <title>Jiajin Tang</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Styles -->
    <style>
          body {
            font-family: 'sans-serif';
            font-size: 16px;
            background-color: #FFFFFF;
            color: #4F6071;
          }
          #header {
            background-color: #f4f4f4;
            /*background-color: #FFFFFF;*/
            display: flex;
            align-items: flex-end;
            padding-top:60px;
            padding-bottom:60px;
          }
          #footer {
            background-color: #FFFFFF;
            padding:60px;
          }
          #portrait {
            border: 3px solid white;
          }
          #header-text {
            margin-top: 60px;
            margin-left: 220px;
          }
          #header-text-name {
            font-size: 50px;
          }
          #header-text-email {
            font-size: 25px;
            font-style: italic;
          }
          .header-text-desc {
            font-size: 20px;
          }
          .vspace-top {
            margin-top: 30px;
          }
          .vspace-top-news {
              margin-top: 15px;
          }
          .paper-image {
            width: 150px;
          }
          .news-date {
              font-weight: bold;
          }
          .paper-title {
            font-weight: bold;
          }
          .paper-authors {
            font-style: italic;
          }
          .date {
            color: #009688;
            font-weight: 700;
            }
        .conference {
            color: #d32f2f;
        }
        p {
        text-align: justify;
        }
    </style>
</head>

<body>
    <div id='header'>
        <div class='container'>
            <div class='row'>
                <div class="col-sm-3 offset-sm-1">
                    <img src='docs/Jiajin.png' class='img-fluid'>
                    
                </div>

                <div class="col">
                  <div id='header-text-name'>
                      Jiajin Tang   ÂîêÂòâÊôã
                  </div>
                  <div id='header-text-email'>
                        tangjj@shanghaitech.edu.cn
                  </div>
                   <div>
                    <a href="https://github.com/Toneyaya">[GitHub]</a>
                    <a href="https://scholar.google.com/citations?user=MVhF6lgAAAAJ&hl=en">[Google Scholar]</a>
                    <!-- <a href="docs/Anpei_CV.pdf">[Download CV]</a> -->
                  </div> 

                </div>
            </div>
        </div>
    </div>


    <div class='container'>
        <div class='row vspace-top'>
            <div class='col offset-sm-1'>
                <h1>Bio</h1>
                <p>
                    I am a PhD student (from 2023 to 2026) at the <a href="https://faculty.sist.shanghaitech.edu.cn/yangsibei">Soolab</a> in <a
                        href="https://www.shanghaitech.edu.cn/"> ShanghaiTech University </a> advised by Prof. Sibei Yang.
                    Previously, I obtained my Bachelor's degree from the <a href=http://www.swu.edu.cn />SouthWest University</a> in 2021.
                    My research interests lie in computer vision, natural language processing, and their intersection. My current research
                    focuses on Vision-Language Understanding and Large Vision-Language Models.
                        <br>
                </p>

                <!-- <div class='vspace-top'>
                    <h1>Services</h1>
                    Area Chair: CVPR 2023/24, 3DV 24<br>
                    Journal reviewer: TOG, TIP, TPAMI, INFFUS ...<br>
                    Conference reviewer: SIGGRAPH, SIGGRAPH Asia, ICCV, ICLR, NeurIPS, AAAI ...
                </div> -->

                <h1>News</h1>
                <p>
                    <span class="date">2025-06</span>: 4 papers are accepted by <span class="conference"> <b>ICCV 2025! üéâüéâüéâüéâ</b></span>
                </p>

                <div class='vspace-top'>
                    <h1>Publications</h1>
                </div>

                <div class='row vspace-top'>
                    <!-- <div class="col-sm-3">
                        <img src='part2object/framework.png' class='img-fluid'>
                    </div> -->

                    <div class="col">
                        <div class='paper-title'>
                            Closed-Loop Transfer for Weakly-supervised Affordance Grounding
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>ICCV 2025</b></span>
                        </div>
                        <div class='paper-authors'>
                            Jiajin Tang*, Zhengxuan Wei*, Ge Zheng, Sibei Yang
                        </div>
                        <div>
                            <a href="https://www.arxiv.org/abs/2407.10084v1">[Paper]</a>
                            <!-- <a href="https://toneyaya.github.io/ddcot/">[Project]</a> -->
                            <a href="https://github.com/ChengShiest/Part2Object">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <!-- <div class="col-sm-3">
                        <img src='part2object/framework.png' class='img-fluid'>
                    </div> -->

                    <div class="col">
                        <div class='paper-title'>
                            Sim-DETR: Unlock DETR for Temporal Sentence Grounding
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>ICCV 2025</b></span>
                        </div>
                        <div class='paper-authors'>
                            Jiajin Tang*, Zhengxuan Wei*, Yuchen Zhu, Cheng Shi, Guanbin Li, Liang Lin, Sibei Yang
                        </div>
                        <div>
                            <a href="https://www.arxiv.org/abs/2407.10084v1">[Paper]</a>
                            <!-- <a href="https://toneyaya.github.io/ddcot/">[Project]</a> -->
                            <a href="https://github.com/ChengShiest/Part2Object">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <!-- <div class="col-sm-3">
                        <img src='part2object/framework.png' class='img-fluid'>
                    </div> -->

                    <div class="col">
                        <div class='paper-title'>
                            Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>ICCV 2025</b></span>
                        </div>
                        <div class='paper-authors'>
                            Zhengxuan Wei*, <b>Jiajin Tang*</b>, Sibei Yang
                        </div>
                        <div>
                            <a href="https://www.arxiv.org/abs/2407.10084v1">[Paper]</a>
                            <!-- <a href="https://toneyaya.github.io/ddcot/">[Project]</a> -->
                            <a href="https://github.com/ChengShiest/Part2Object">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <!-- <div class="col-sm-3">
                        <img src='part2object/framework.png' class='img-fluid'>
                    </div> -->

                    <div class="col">
                        <div class='paper-title'>
                            Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>ICCV 2025</b></span>
                        </div>
                        <div class='paper-authors'>
                            Ge Zheng*, Jiaye Qian*, <b>Jiajin Tang</b>, Sibei Yang
                        </div>
                        <div>
                            <!-- <a href="https://www.arxiv.org/abs/2407.10084v1">[Paper]</a> -->
                            <!-- <a href="https://toneyaya.github.io/ddcot/">[Project]</a> -->
                            <!-- <a href="https://github.com/ChengShiest/Part2Object">[Code]</a> -->
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='part2object/framework.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Part2Object: Hierarchical Unsupervised 3D Instance Segmentation
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>ECCV 2024</b></span>
                        </div>
                        <div class='paper-authors'>
                            Cheng Shi*, Yulin Zhang*, Bin Yang, <b>Jiajin Tang</b>, Yuexin Ma, Sibei Yang
                        </div>
                        <div>
                            <a href="https://www.arxiv.org/abs/2407.10084v1">[Paper]</a>
                            <!-- <a href="https://toneyaya.github.io/ddcot/">[Project]</a> -->
                            <a href="https://github.com/ChengShiest/Part2Object">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='ddcot/img/framework.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>NeurIPS 2023</b></span>
                        </div>
                        <div class='paper-authors'>
                            Ge Zheng*, Bin Yang*, <b>Jiajin Tang*</b>, Hong-Yu Zhou, Sibei Yang
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2310.16436">[Paper]</a>
                            <a href="https://toneyaya.github.io/ddcot/">[Project]</a>
                            <a href="https://github.com/SooLab/DDCOT">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='tempcd/img/tempcd.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Temporal Collection and Distribution for Referring Video Object Segmentation
                        </div>
                        <div class='paper-desc'>
                             <span class="conference"> <b>ICCV 2023</b></span>
                        </div>
                        <div class='paper-authors'>
                            <b>Jiajin Tang</b>, Ge Zheng, Sibei Yang
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2309.03473">[Paper]</a>
                            <a href="https://toneyaya.github.io/tempcd/">[Project]</a>
                            <!-- <a href="https://github.com/Toneyaya/tempcd">[Code]</a> -->
                        </div>
                    </div>
                </div>


               <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='cotdet/img/cotdet.png' class='img-fluid'>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection
                      </div>
                      <div class='paper-desc'>
                        <span class="conference"> <b>ICCV 2023</b></span>
                      </div>
                      <div class='paper-authors'>
                        <b>Jiajin Tang*</b>, Ge Zheng*, Jingyi Yu, Sibei Yang
                      </div>
                      <div>
                        <a href="https://arxiv.org/abs/2309.01093">[Paper]</a>
                        <a href="https://toneyaya.github.io/cotdet/">[Project]</a>
                        <a href="https://github.com/Toneyaya/CoTDet">[Code]</a>
                    </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src="docs/3338.png" class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Contrastive Grouping with Transformer for Referring Image Segmentation
                        </div>
                        <div class='paper-desc'>
                            <span class="conference"> <b>CVPR 2023</b></span>
                        </div>
                        <div class='paper-authors'>
                            <b>Jiajin Tang</b>, Ge Zheng, Cheng Shi, Sibei Yang
                        </div>
                        <div>
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf?ref=https://githubhelp.com">[Paper]</a>
                            <a href="https://github.com/Toneyaya/CGFormer">[Code]</a>
                        </div>
                    </div>
                </div>



            </div>
        </div>
    </div>



                

    <div id='footer' class='vspace-top'>
    <div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
</body>

</html>
