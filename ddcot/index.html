<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="">
    <meta name="author" content="Ge Zheng*,
                                Bin Yang*,
                                Jiajin Tang*,
                                Hong-Yu Zhou,
                                Sibei Yang">

    <title>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">DDCoT</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</h1>
<!--    <h2>TensoRF: Tensorial Radiance Fields</h2>-->
        <h3 class="nerf_subheader_v2">Accepted by NeurIPS 2023</h3>
<!--            <p class="abstract">A compact and efficent scene representation</p>-->
    <hr>
    <p class="authors">
        <a> Ge Zheng*</a>,
        <a> Bin Yang*</a>,
        <a href="https://Toneyaya.github.io/"> Jiajin Tang*</a>,
        <a href="https://zhouhy.org/"> Hong-Yu Zhou</a>,
        <a href="https://faculty.sist.shanghaitech.edu.cn/"> Sibei Yang✉</a>

    </p>

    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>denotes equal contribution.</div>

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2310.16436">Paper</a>
        <a class="btn btn-primary" href="https://github.com/SooLab/DDCOT">Code</a>
    </div>
</div>



<div class="container">
    <!-- <div class="w-container">
        <h2 class="grey-heading_nerf">Our insights</h2>
        </br></br>
        <div class="vcontainer" style="text-align: center;">
            <img width=80% height=80% center src="./img/intro.png" alt="Dask Driven Object Detection with visual affordance knowledge">
        </div>
    </div>
    </br> -->
<!--<hr>-->
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                A long-standing goal of AI systems is to perform complex multimodal reasoning
                like humans. Recently, large language models (LLMs) have made remarkable
                strides in such multi-step reasoning on the language modality solely by leveraging
                the chain of thought (CoT) to mimic human thinking. However, the transfer of these
                advancements to multimodal contexts introduces heightened challenges, including
                but not limited to the impractical need for labor-intensive annotation and the
                limitations in terms of flexibility, generalizability, and explainability. To evoke CoT
                reasoning in multimodality, this work first conducts an in-depth analysis of these
                challenges posed by multimodality and presents two key insights: “keeping critical
                thinking” and “letting everyone do their jobs” in multimodal CoT reasoning.
                Furthermore, this study proposes a novel DDCoT prompting that maintains a
                critical attitude through negative-space prompting and incorporates multimodality
                into reasoning by first dividing the reasoning responsibility of LLMs into reasoning
                and recognition and then integrating the visual recognition capability of visual
                models into the joint reasoning process. The rationale generated by DDCoT not
                only improves the reasoning abilities of both large and small language models in
                zero-shot prompting and fine-tuning learning, significantly outperforming state-of-
                the-art methods but also exhibits impressive generalizability and explainability.
            </p>
 
            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <!-- <p class="paragraph-3 nerf_text">
                Given a task (e.g., “open parcel”) and an image, task driven object detection requires detecting a set of objects most preferred to afford the
                task.
                Note that the target objects indicated by the task are not fixed in quantity and category, which may vary with changes
                in the image scene. In contrast, traditional object detection detects objects of fixed categories while referring image
                grounding localizes unambiguous objects.
            </p> -->


            <div class="columns-5 w-row">
                <img src="img/framework.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text">
                An overview of our DDCoT and its utilization to improve the multimodal reasoning of
                LMs. Note that although errors encounter in the second sub-problem during visual recognition, the
                language model rectifies this error in the joint reasoning step with critical thought
            </p>
        </div>
    </div>



     </br></br>
        <div class="section">
            <s2>Performance</s2>

            <p class="paragraph-3 nerf_text">
                Table below shows the comparison of our DDCoT with the state-of-the-art models on zero-shot
                and fine-tuning benchmarks. Our approach consistently achieves superior performance compared to
                previous methods. Note that we further report the results of the recent works which have
                not been published but have been preprinted in the research community.
            </p>

            <!-- <h2 class="grey-heading_nerf">
                Main Results
            </h2>
            <p class="paragraph-3 nerf_text">
                Compared to TOIST, our CoTDet achieves significant performance improvement (15.6% mAPbox and 14.8% mAPmask), which
                demonstrates the effectiveness of our task-relevant knowledge acquisition and utilization. Compared to the two-stage
                method GGNN, we achieve 24.3% mAPbox and 21.2% mAPmask performance gain, which demonstrates the importance of leveraging
                the visual affordance knowledge rather than purely visual context information.
            </p> -->
            <div class="columns-5 w-row">
                <img src="img/DDCoT_results.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>
            <p class="paragraph-3 nerf_text">
                Main results (%). Size = backbone model size. GT-R means models are trained with ground
                truth rationales. Question classes: NAT = natural science, SOC = social science, LAN = language
                science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 =
                grades 7-12. † denotes implementation by removing ground truth rationales when fine-tuning
            </p>
            <!-- <div class="columns-5 w-row">
                <img src="img/e2.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div> -->

            <h2 class="grey-heading_nerf">
                Visualization
            </h2>
            <p class="paragraph-3 nerf_text">
                Here we visualize several qualitative results. 
            </p>
            <div class="columns-5 w-row">
                <img src="img/case1.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text">
                Showcases several map-related questions, demonstrating how our method integrates simple visual
                features (such as the shape of highlighted areas) with common knowledge to obtain correct reasoning
                and answers.
            </p>

            <p class="paragraph-3 nerf_text">
                In the examples presented below, our method successfully identifies within the images, acquiring relevant knowledge
            </p>

            <div class="columns-5 w-row">
                <img src="img/case2.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>
            
            <p class="paragraph-3 nerf_text">
                Illustrates four more complex questions, where our method leverages information obtained from the images to perform intricate reasoning. However,
                when it comes to the complex interaction between images and textual context, our method still fall
                into hallucinations, leading to erroneous reasoning and answers.
            </p>
            
            <div class="columns-5 w-row">
                <img src="img/case3.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>


            <!-- <p class="paragraph-3 nerf_text nerf_results_text">
                The (c) and (d) demonstrate effectiveness without MLCOT or knowledge-conditional denoising training (KDN). With visual
                affordance knowledge obtained by directly asking LLMs, our model relies solely on matching with the single knowledge
                unit, which incorrectly detects the trunk in (c) and misses the knife in (d). The former trunk is easily confused with
                objects that are “flat, broad with a handle”, while the latter knife is ignored because its visual attributes of
                straight mismatch the single knowledge unit that includes “curved or angled”. Furthermore, without KDN, our detector
                lacks explicit guidance, leading to inaccurate detection in challenging scenes. Specifically, the glove in (c) and the
                knife in (d) are not detected successfully, and the packing line in (d) is mistakenly detected.
            </p> -->


<!-- 
            </br></br></br>
            <s2> Acknowledgements </s2>
            <hr>
            <p class="paragraph-3 nerf_text">
                We would like to thank .
            </p> -->

        </br></br>
<!--    <div class="section">-->
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
    @INPROCEEDINGS{Zheng_NeurIPS2023,
        author = {Ge Zheng and Bin Yang and Jiajin Tang and Hong-Yu Zhou and Sibei Yang},
        title = {DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models},
        booktitle = {NeurIPS2023},
        year = {2023}
    }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from NeRF.
        </p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
